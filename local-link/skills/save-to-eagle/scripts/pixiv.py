#!/usr/bin/env python3
"""
Pixiv ä½œå“å½’æ¡£åˆ° Eagle
"""
import json
import re
import requests
from pathlib import Path
from urllib.parse import urlparse
from eagle_utils import (
    LIBRARY_ROOT,
    FOLDER_IDS,
    create_eagle_asset,
    create_subfolder,
    rebuild_mtime_index,
    sanitize_filename,
    download_image
)

# Pixiv cookies æ–‡ä»¶è·¯å¾„
COOKIES_PATH = LIBRARY_ROOT / ".secrets" / "pixiv_cookies.json"


def load_cookies():
    """åŠ è½½ Pixiv cookies"""
    if not COOKIES_PATH.exists():
        raise FileNotFoundError(
            f"Pixiv cookies æ–‡ä»¶ä¸å­˜åœ¨: {COOKIES_PATH}\n"
            "è¯·ä»æµè§ˆå™¨å¯¼å‡º cookies å¹¶ä¿å­˜åˆ°è¯¥ä½ç½®"
        )

    cookies_data = json.loads(COOKIES_PATH.read_text())
    return {c["name"]: c["value"] for c in cookies_data}


def extract_artwork_id(url: str) -> str:
    """ä» URL æå–ä½œå“ ID"""
    patterns = [
        r"pixiv\.net/artworks/(\d+)",
        r"pixiv\.net/member_illust\.php.*illust_id=(\d+)",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    raise ValueError(f"æ— æ³•ä» URL æå–ä½œå“ ID: {url}")


def fetch_artwork_info(artwork_id: str) -> dict:
    """è·å– Pixiv ä½œå“ä¿¡æ¯"""
    cookies = load_cookies()

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
        "Referer": "https://www.pixiv.net/",
    }

    # è·å–ä½œå“è¯¦æƒ…
    ajax_url = f"https://www.pixiv.net/ajax/illust/{artwork_id}"
    resp = requests.get(ajax_url, headers=headers, cookies=cookies)
    resp.raise_for_status()

    data = resp.json()
    if data.get("error"):
        raise Exception(f"Pixiv API é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")

    illust = data["body"]

    return {
        "id": artwork_id,
        "title": illust["illustTitle"],
        "author": illust["userName"],
        "author_id": illust["userId"],
        "page_count": illust["pageCount"],
        "urls": illust["urls"],
    }


def fetch_artwork_pages(artwork_id: str) -> list:
    """è·å–å¤šå›¾ä½œå“çš„æ‰€æœ‰é¡µé¢ URL"""
    cookies = load_cookies()

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
        "Referer": "https://www.pixiv.net/",
    }

    pages_url = f"https://www.pixiv.net/ajax/illust/{artwork_id}/pages"
    resp = requests.get(pages_url, headers=headers, cookies=cookies)
    resp.raise_for_status()

    data = resp.json()
    if data.get("error"):
        raise Exception(f"Pixiv API é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")

    return [page["urls"]["original"] for page in data["body"]]


def archive_pixiv(url: str, star: int = 0, single: bool = False):
    """
    å½’æ¡£ Pixiv ä½œå“åˆ° Eagle

    å•å›¾ï¼šç›´æ¥æ”¾å…¥ Pixiv æ–‡ä»¶å¤¹ï¼Œæ–‡ä»¶å = æ ‡é¢˜
    å¤šå›¾ï¼šåˆ›å»ºå­æ–‡ä»¶å¤¹ï¼Œå›¾ç‰‡å‘½åä¸º p1, p2...
    å¤šå›¾ + single=Trueï¼šåªä¸‹è½½ç¬¬ä¸€å¼ å›¾ï¼Œç›´æ¥æ”¾å…¥ Pixiv æ–‡ä»¶å¤¹

    Args:
        url: ä½œå“é“¾æ¥
        star: è¯„åˆ†ï¼ˆ1-5æ˜Ÿï¼Œ0è¡¨ç¤ºæ— è¯„åˆ†ï¼‰
        single: ä»…ä¸‹è½½ç¬¬ä¸€å¼ å›¾ï¼ˆä»… Pixiv å¤šå›¾ä½œå“æœ‰æ•ˆï¼‰
    """
    print(f"ğŸ¨ æ­£åœ¨å½’æ¡£ Pixiv ä½œå“...")
    print(f"   URL: {url}")

    # æå–ä½œå“ ID
    artwork_id = extract_artwork_id(url)
    print(f"   ä½œå“ ID: {artwork_id}")

    # è·å–ä½œå“ä¿¡æ¯
    info = fetch_artwork_info(artwork_id)
    title = info["title"]
    author = info["author"]
    page_count = info["page_count"]

    print(f"   æ ‡é¢˜: {title}")
    print(f"   ä½œè€…: {author}")
    print(f"   é¡µæ•°: {page_count}")

    # å‡†å¤‡ä¸‹è½½
    pixiv_folder_id = FOLDER_IDS["Pixiv"]
    temp_dir = Path("/tmp/pixiv_download")
    temp_dir.mkdir(exist_ok=True)

    downloaded = []

    if page_count == 1 or single:
        # å•å›¾æ¨¡å¼ï¼šç›´æ¥æ”¾å…¥ Pixiv æ–‡ä»¶å¤¹
        if page_count == 1:
            print(f"\nğŸ“¥ å•å›¾æ¨¡å¼ï¼Œç›´æ¥å½’æ¡£åˆ° Pixiv æ–‡ä»¶å¤¹")
            image_url = info["urls"]["original"]
        else:
            print(f"\nğŸ“¥ å¤šå›¾ä½œå“ï¼Œä»…ä¸‹è½½ç¬¬ä¸€å¼ å›¾")
            # è·å–ç¬¬ä¸€å¼ å›¾çš„ URL
            page_urls = fetch_artwork_pages(artwork_id)
            image_url = page_urls[0]

        ext = image_url.split(".")[-1].split("?")[0]
        temp_path = temp_dir / f"image.{ext}"

        download_image(image_url, temp_path, headers={"Referer": "https://www.pixiv.net/"})

        safe_title = sanitize_filename(title)
        metadata = create_eagle_asset(
            image_path=temp_path,
            name=safe_title,
            folder_id=pixiv_folder_id,
            source_url=url,
            annotation=f"ä½œè€…: {author}",
            tags=[],
            star=star
        )

        downloaded.append({
            "name": safe_title,
            "width": metadata["width"],
            "height": metadata["height"],
            "size": metadata["size"]
        })

        print(f"   âœ… {safe_title}.{ext}")

    else:
        # å¤šå›¾ï¼šåˆ›å»ºå­æ–‡ä»¶å¤¹
        print(f"\nğŸ“¥ å¤šå›¾æ¨¡å¼ï¼Œåˆ›å»ºå­æ–‡ä»¶å¤¹")

        safe_folder_name = sanitize_filename(f"{author} - {title}", max_len=60)
        subfolder_id = create_subfolder(
            pixiv_folder_id,
            safe_folder_name,
            description=f"ä½œè€…: {author}"
        )
        print(f"   åˆ›å»ºæ–‡ä»¶å¤¹: {safe_folder_name}")

        # è·å–æ‰€æœ‰é¡µé¢ URL
        page_urls = fetch_artwork_pages(artwork_id)

        for i, image_url in enumerate(page_urls, 1):
            ext = image_url.split(".")[-1].split("?")[0]
            temp_path = temp_dir / f"p{i}.{ext}"

            download_image(image_url, temp_path, headers={"Referer": "https://www.pixiv.net/"})

            metadata = create_eagle_asset(
                image_path=temp_path,
                name=f"p{i}",
                folder_id=subfolder_id,
                source_url=url,
                annotation=f"ä½œè€…: {author}",
                tags=[],
                star=star
            )

            downloaded.append({
                "name": f"p{i}",
                "width": metadata["width"],
                "height": metadata["height"],
                "size": metadata["size"]
            })

            print(f"   âœ… p{i}: {metadata['width']}Ã—{metadata['height']}")

    # é‡å»ºç´¢å¼•
    rebuild_mtime_index()

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import shutil
    shutil.rmtree(temp_dir, ignore_errors=True)

    # è¿”å›ç»“æœ
    return {
        "platform": "Pixiv",
        "title": title,
        "author": author,
        "url": url,
        "page_count": page_count,
        "downloaded": downloaded
    }
